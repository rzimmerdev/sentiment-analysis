{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install lightning\n",
    "!pip install scipy==1.12\n",
    "!pip install gensim"
   ],
   "metadata": {
    "id": "t_RvPai5zygO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Redes Neurais e Aprendizado Profundo - Trabalho Final\n",
    "\n",
    "## Grupo:\n",
    "| Nome            | nUSP     |\n",
    "|-----------------|----------|\n",
    "| Rafael Zimmer   | 12542612 |\n",
    "| Murilo Soave    | 10688813 |\n",
    "| Fernando Cesar  | 10260559 |\n",
    "\n",
    "## Tarefa:\n",
    "**Classificação**\n",
    "\n",
    "## Dados:\n",
    "\"*Sentiment Analysis for Financial News*\"\n",
    "\n",
    "## GitHub com resultados:\n",
    "https://github.com/rzimmerdev/sentiment-analysis"
   ],
   "metadata": {
    "id": "bZu8jo6iFnb8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Descrição da Tarefa\n",
    "\n",
    "A tarefa consiste em realizar a análise de sentimentos em um conjunto de dados de frases financeiras. O objetivo é classificar cada frase como positiva, negativa ou neutra (3 classes, ou seja, multi-class single-label) em relação ao seu conteúdo emocional. Para isso, utilizamos três abordagens diferentes de modelagem de forma comparativa:\n",
    "- uma abordagem baseline com Bag of Words;\n",
    "- uma abordagem state-of-the-art com um Transformer pré-treinado (BERT);\n",
    "- uma abordagem adicional utilizando Word2Vec.\n",
    "\n",
    "## Dataset Escolhido\n",
    "O dataset escolhido para esta análise é o FinancialPhraseBank, disponivel no [Kaggle](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news).\n",
    "\n",
    "É um conjunto de dados relativamente popular utilizado na análise de sentimentos financeiros. Este dataset contém frases extraídas de relatórios financeiros e principalmente de artigos de notícias.\n",
    "\n",
    "É um dataset relativamente simples e pré-processado (até certo ponto, é necessário algumas transformações, principalmente para nossos modelos). Não é tão balanceado (59% positivas, 28% neutras e 12% negativas).\n",
    "\n",
    "## Abordagem Adotada\n",
    "\n",
    "Para a abordagem baseline, utilizamos um modelo Bag of Words (BoW), que transforma cada frase em um vetor de frequências de palavras, ignorando a ordem e o contexto das palavras, pois foi introduzido durante as aulas e também é usado comumente para problemas beeem simples que envolvam poucas classes ou dados não muito complexos.\n",
    "\n",
    "Para a abordagem state of the art (SOA) escolhemos uma arquitetura de Transformer que tem capacidade de processar palavras em contexto, que é extremamente recorrente quando se trata de sentimentos em texto (o sentimento geralmente é definido por algumas palavras mas que dependem extremamente do contexto, por exemplo: \"O cenário econômico está extremamente volátil, mas a Apple performou bem.\", o sentimento é positivo ou negativo, depende se o contexto é o mercado ou a empresa Apple).\n",
    "\n",
    "Para a abordagem adicional, escolhemos o W2V, e utilizamos uma rede recorrente (LSTM, especificamente) para a classificação. Essa escolha se dá pois é um bom ponto intermediário para os dois outros modelos, além de utilizar redes recorrentes que foi outro tópico abordado em aula."
   ],
   "metadata": {
    "id": "5HxPdVgVEg6z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from lightning import LightningModule, Trainer\n",
    "from torchmetrics import Accuracy\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, f1_score, accuracy_score, log_loss"
   ],
   "metadata": {
    "id": "a_8myhu3zvP-"
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "K8MSa8gzzfST"
   },
   "outputs": [],
   "source": [
    "class PhraseDataset(Dataset):\n",
    "    def __init__(self, x_col, y_col, tokenizer: torch.nn.Module, data: pd.DataFrame, max_len: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "\n",
    "        self.num_classes = len(self.data[self.y_col].unique())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        phrase = str(self.data.loc[item, self.x_col])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            phrase,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        label = self.data.loc[item, self.y_col]\n",
    "        target = F.one_hot(torch.tensor(label, dtype=torch.long), num_classes=self.num_classes)\n",
    "\n",
    "        return {\n",
    "            \"phrase\": phrase,\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"target\": target.float()\n",
    "        }\n",
    "\n",
    "\n",
    "class Splitter:\n",
    "    def __init__(self, train_size=0.8, seed=42, folder='data'):\n",
    "        self.train_size = train_size\n",
    "        self.seed = seed\n",
    "        self.folder = folder\n",
    "\n",
    "    @property\n",
    "    def paths(self):\n",
    "        folder_path = self.folder.split('/')\n",
    "        if len(folder_path) > 1:\n",
    "            folder = \"/\".join(folder_path[:-1])\n",
    "        else:\n",
    "            folder = folder_path[0]\n",
    "\n",
    "        train_path = f'{folder}/train.csv'\n",
    "        test_path = f'{folder}/test.csv'\n",
    "\n",
    "        return train_path, test_path\n",
    "\n",
    "    def split(self, df: pd.DataFrame):\n",
    "        indices = torch.randperm(len(df)).tolist()\n",
    "\n",
    "        train_size = int(self.train_size * len(df))\n",
    "\n",
    "        train_indices = indices[:train_size]\n",
    "        test_indices = indices[train_size:]\n",
    "\n",
    "        train_df = df.loc[train_indices]\n",
    "        test_df = df.loc[test_indices]\n",
    "\n",
    "        train_path, test_path = self.paths\n",
    "\n",
    "        train_df.to_csv(train_path, index=False, header=False)\n",
    "        test_df.to_csv(test_path, index=False, header=False)\n",
    "\n",
    "        return train_path, test_path\n",
    "\n",
    "\n",
    "class FinancialPhraseDataset(PhraseDataset):\n",
    "    \"\"\"\n",
    "    __getitem__ returns a dictionary with the following keys:\n",
    "    - phrase: the original phrase\n",
    "    - input_ids: the tokenized phrase\n",
    "    - attention_mask: the attention mask\n",
    "    - target: the target sentiment\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer: torch.nn.Module = None,\n",
    "                 path='data/archive.zip',\n",
    "                 filename='data/all-data.csv',\n",
    "                 max_len=512,\n",
    "                 seed=None):\n",
    "        splitter = Splitter(seed=seed)\n",
    "\n",
    "        train_path, test_path = splitter.paths\n",
    "\n",
    "        try:\n",
    "            self.data = pd.read_csv(train_path, header=None, encoding='latin-1')\n",
    "            self.test_data = pd.read_csv(test_path, header=None, encoding='latin-1')\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                import zipfile\n",
    "                with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall('data')\n",
    "                self.data = pd.read_csv(filename, header=None, encoding='latin-1')\n",
    "            except FileNotFoundError:\n",
    "                raise FileNotFoundError(\"File not found. Please download the dataset from Kaggle.\")\n",
    "            splitter.split(self.data)\n",
    "\n",
    "            self.data = pd.read_csv(train_path, header=None, encoding='latin-1')\n",
    "            self.test_data = pd.read_csv(test_path, header=None, encoding='latin-1')\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        super().__init__(x_col=1, y_col=0, tokenizer=tokenizer, data=self.data, max_len=max_len)\n",
    "        self.data = self.preprocess(self.data)\n",
    "        self.test_data = self.preprocess(self.test_data)\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        data[0] = data[0].map({'neutral': 0, 'positive': 1, 'negative': 2})\n",
    "        data = data.dropna()\n",
    "        data = data.reset_index(drop=True)\n",
    "        return data\n",
    "\n",
    "    def get_data_loaders(self, batch_size=8, shuffle=True, num_workers=0, train_size=0.8, train=True):\n",
    "        if train:\n",
    "            train_size = int(train_size * len(self.data))\n",
    "            val_size = len(self.data) - train_size\n",
    "\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(self, [train_size, val_size])\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                num_workers=num_workers\n",
    "            )\n",
    "\n",
    "            val_loader = torch.utils.data.DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=num_workers\n",
    "            )\n",
    "\n",
    "            return train_loader, val_loader\n",
    "        else:\n",
    "            test_dataset = PhraseDataset(x_col=1,\n",
    "                                         y_col=0,\n",
    "                                         tokenizer=self.tokenizer,\n",
    "                                         data=self.test_data,\n",
    "                                         max_len=self.max_len)\n",
    "\n",
    "            test_loader = torch.utils.data.DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=num_workers\n",
    "            )\n",
    "\n",
    "            return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bag of Words\n",
    "\n",
    "A nossa implementação tem dois pontos importantes:\n",
    "- A classe de vetorização, que utiliza uma função de contagem de frequência como entrada do modelo de classificação.\n",
    "- O modelo de classificação em si, que é apenas uma rede neural totalmente conectada (pesos lineares) com uma saída em porcentagem.\n",
    "\n",
    "Essa abordagem é extremamente simples, baseada no teorema de Bayes, em que a frequência é utilizada como o Prior.\n",
    "\n",
    "![BOW vector](https://uc-r.github.io/public/images/analytics/feature-engineering/bow-image.png)"
   ],
   "metadata": {
    "id": "RiusqVXVH0kn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BoWVectorizer:\n",
    "    def __init__(self, max_features=4000):\n",
    "        self.vectorizer = CountVectorizer(max_features=int(max_features))\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        return self.vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "    def transform(self, texts):\n",
    "        return self.vectorizer.transform(texts).toarray()\n",
    "\n",
    "    def save(self, path):\n",
    "        import pickle\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.vectorizer, f)\n",
    "\n",
    "    def load(self, path):\n",
    "        import pickle\n",
    "        with open(path, 'rb') as f:\n",
    "            self.vectorizer = pickle.load(f)\n",
    "\n",
    "\n",
    "class BowClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=5, output_dim=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(hidden_layers - 1)],\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim // 2, output_dim),\n",
    "            self.softmax\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class LitBowClassifier(LightningModule):\n",
    "    def __init__(self, input_dim, hidden_layers=5, lr=1e-3):\n",
    "        super().__init__()\n",
    "        input_dim = int(input_dim)\n",
    "        self.model = BowClassifier(input_dim, hidden_layers=hidden_layers)\n",
    "        self.lr = lr\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=3)\n",
    "        self.vectorizer = BoWVectorizer(max_features=input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        texts = batch['phrase']\n",
    "        target = batch['target']\n",
    "\n",
    "        # Transform texts to BoW vectors\n",
    "        input_ids = torch.tensor(self.vectorizer.transform(texts), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        output = self(input_ids)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        acc = self.accuracy(torch.argmax(output, dim=1), torch.argmax(target, dim=1))\n",
    "\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        texts = batch['phrase']\n",
    "        target = batch['target']\n",
    "\n",
    "        # Transform texts to BoW vectors\n",
    "        input_ids = torch.tensor(self.vectorizer.transform(texts), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        output = self(input_ids)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        acc = self.accuracy(output, target)\n",
    "\n",
    "        return output, target, loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def save(self, model_path, vectorizer_path):\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        self.vectorizer.save(vectorizer_path)\n",
    "\n",
    "    def load(self, model_path, vectorizer_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.vectorizer.load(vectorizer_path)\n",
    "\n",
    "    def fit_vectorizer(self, train_texts):\n",
    "        self.vectorizer.fit_transform(train_texts)"
   ],
   "metadata": {
    "id": "hcwhpnsa-PLr"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word To Vector\n",
    "\n",
    "O W2V é em dificuldade de implementação um pouco mais complexo do que o BOW puro, devido à necessidade de criar um espaço latente de embeddings (representação vetorial das palavras, ou seja, transformar as palavras em números).\n",
    "\n",
    "Nossa implementação conta com o método de vetorização, similar ao do BOW, mas utilizado a biblioteca Gensim que tem uma implementação para a criação do espaço automaticamente utilizando as frases existentes.\n",
    "\n",
    "![W2V embeddings](https://cdn.coveo.com/images/w_1200,h_700,c_scale/v1707326301/blogprod/WordEmbeddings_106321438d/WordEmbeddings_106321438d.png?_i=AA)"
   ],
   "metadata": {
    "id": "k11_1yeZId7B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Word2VecVectorizer:\n",
    "    def __init__(self, embedding_dim=2000):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        self.model = Word2Vec(sentences, vector_size=self.embedding_dim, window=5, min_count=1, workers=4)\n",
    "\n",
    "    def transform(self, sentences):\n",
    "        embeddings = [torch.tensor(np.array([self.model.wv[word] for word in sentence if word in self.model.wv]), dtype=torch.float32)\n",
    "                      for sentence in sentences]\n",
    "        return pad_sequence(embeddings, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = Word2Vec.load(path)\n",
    "\n",
    "\n",
    "class Word2VecClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            self.softmax\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return self.classifier(hidden[-1])\n",
    "\n",
    "\n",
    "class LitWord2VecClassifier(LightningModule):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim=3, lr=1e-3, num_layers=8):\n",
    "        super().__init__()\n",
    "        self.model = Word2VecClassifier(embedding_dim, hidden_dim, output_dim, num_layers)\n",
    "        self.lr = lr\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=output_dim)\n",
    "        self.vectorizer = Word2VecVectorizer(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        texts = batch['phrase']\n",
    "        target = batch['target']\n",
    "\n",
    "        # Transform texts to Word2Vec embeddings\n",
    "        input_ids = self.vectorizer.transform(texts).to(self.device)\n",
    "\n",
    "        output = self(input_ids)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        acc = self.accuracy(torch.argmax(output, dim=1), torch.argmax(target, dim=1))\n",
    "\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        texts = batch['phrase']\n",
    "        target = batch['target']\n",
    "\n",
    "        # Transform texts to Word2Vec embeddings\n",
    "        input_ids = self.vectorizer.transform(texts).to(self.device)\n",
    "\n",
    "        output = self(input_ids)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        acc = self.accuracy(output, target)\n",
    "\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('test_acc', acc, on_step=True, on_epoch=True)\n",
    "\n",
    "        pred = output.argmax(dim=1)\n",
    "        self.log('test_pred', pred, on_step=True, on_epoch=True)\n",
    "        self.log('test_target', target, on_step=True, on_epoch=True)\n",
    "\n",
    "        return output, target, loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def save(self, model_path, vectorizer_path):\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        self.vectorizer.save(vectorizer_path)\n",
    "\n",
    "    def load(self, model_path, vectorizer_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.vectorizer.load(vectorizer_path)\n",
    "\n",
    "    def fit_vectorizer(self, train_texts):\n",
    "        tokenized_texts = [text.split() for text in train_texts]\n",
    "        self.vectorizer.fit(tokenized_texts)"
   ],
   "metadata": {
    "id": "Jxvtxy72-Df0"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer com Transfer Learning\n",
    "\n",
    "O Transformer é um modelo de rede neural que foi introduzido em 2017 e é extremamente eficaz para tarefas de NLP, pois\n",
    "consegue entender o contexto de uma palavra observando tanto o que vem antes quanto o que vem depois dela (bidirecional).\n",
    "\n",
    "Para a nossa implementação, utilizamos o modelo Bert-Small, que é uma versão mais leve do BERT, mas com a mesma\n",
    "arquitetura. Congelamos os pesos do modelo e inserimos camadas adicionais para a classificação, que é uma forma de\n",
    "transfer learning, ou seja, treinar um modelo em cima de outro modelo já treinado."
   ],
   "metadata": {
    "id": "KuZjqtEBJm1a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_layers=5, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "\n",
    "        # Freeze the BERT model parameters\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # unfreeze last\n",
    "        for param in self.bert.encoder.layer[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sequential = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Linear(embedding_dim, embedding_dim),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(hidden_layers - 1)],\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim // 2, output_dim),\n",
    "            self.softmax\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output.last_hidden_state\n",
    "        return self.sequential(hidden_state[:, 0, :])\n",
    "\n",
    "\n",
    "class LitTransformerClassifier(LightningModule):\n",
    "    def __init__(self, hidden_layers=5, lr=1e-3):\n",
    "        super().__init__()\n",
    "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.model = TransformerClassifier(bert, hidden_layers=hidden_layers, output_dim=3)\n",
    "        self.lr = lr\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target = batch['target']\n",
    "\n",
    "        output = self(input_ids, attention_mask)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        acc = self.accuracy(output, target)\n",
    "\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target = batch['target']\n",
    "\n",
    "        output = self(input_ids, attention_mask)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        acc = self.accuracy(output, target)\n",
    "\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('test_acc', acc, on_step=True, on_epoch=True)\n",
    "\n",
    "        return output, target, loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n"
   ],
   "metadata": {
    "id": "94YwJ-I8zjQv"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_training_history(log_dir, model):\n",
    "    metrics = pd.read_csv(f'{log_dir}/metrics.csv')\n",
    "    plt.figure()\n",
    "\n",
    "    # Make a 1/30 moving average\n",
    "    window = 10\n",
    "    metrics['train_loss_step'] = metrics['train_loss_step'].bfill().rolling(window=window).mean()\n",
    "    metrics['train_acc_step'] = metrics['train_acc_step'].bfill().rolling(window=window).mean()\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Training Loss', color='tab:blue')\n",
    "    ax1.plot(metrics['step'], metrics['train_loss_step'], label='Training Loss', color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Training Accuracy', color='tab:orange')\n",
    "    ax2.plot(metrics['step'], metrics['train_acc_step'], label='Training Accuracy', color='tab:orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title(f'Training History - {model}')\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "    plt.savefig(f'{model}_training_history.png')\n",
    "\n",
    "\n",
    "def train(model, max_epochs=5, batch_size=64, num_workers=4, lr=1e-5):\n",
    "    dataset = FinancialPhraseDataset()\n",
    "    print(f\"Training {model} model\")\n",
    "    print(f\"Maimum number of epochs: {max_epochs}. \"\n",
    "          f\"Batch size: {batch_size}. Number of workers: {num_workers}. Learning rate: {lr}\")\n",
    "    train_loader, _ = dataset.get_data_loaders(batch_size=batch_size, num_workers=num_workers, train_size=1)\n",
    "    lr = float(lr)\n",
    "\n",
    "    if model == 'bow':\n",
    "        model_path = 'checkpoints/bow'\n",
    "        lit_model = LitBowClassifier(2000, 6, lr=lr)\n",
    "        lit_model.fit_vectorizer(dataset.data[1])\n",
    "    elif model == 'w2v':\n",
    "        model_path = 'checkpoints/w2v'\n",
    "        lit_model = LitWord2VecClassifier(embedding_dim=512, hidden_dim=256, num_layers=8, lr=lr)\n",
    "        lit_model.fit_vectorizer(dataset.data[1])\n",
    "    elif model == 'transformer':\n",
    "        model_path = 'checkpoints/transformer'\n",
    "        lit_model = LitTransformerClassifier(hidden_layers=3, lr=lr)\n",
    "    else:\n",
    "        raise ValueError('Unknown model')\n",
    "\n",
    "    log_dir = os.path.join(f'logs/{model}')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    logger = CSVLogger(\"logs\", name=model)\n",
    "    trainer = Trainer(max_epochs=max_epochs, logger=logger)\n",
    "    trainer.fit(model=lit_model, train_dataloaders=train_loader)\n",
    "\n",
    "    # if directory does not exist, create it\n",
    "    if not os.path.exists('checkpoints'):\n",
    "        os.makedirs('checkpoints')\n",
    "\n",
    "    # save model\n",
    "    if model == 'transformer':\n",
    "        trainer.save_checkpoint(model_path + '.ckpt')\n",
    "    elif model == 'bow':\n",
    "        lit_model.save(model_path + '.pth', model_path + '.pkl')\n",
    "    elif model == 'w2v':\n",
    "        lit_model.save(model_path + '.pth', model_path + '.model')\n",
    "\n",
    "    # save results\n",
    "    # list all versions, and access metrics from latest\n",
    "    version = max([int(version.split('_')[-1]) for version in os.listdir(f'logs/{model}')])\n",
    "    log_dir = os.path.join(f'logs/{model}', f'version_{version}')\n",
    "    plot_training_history(log_dir, model)\n",
    "\n",
    "\n",
    "class Models(Enum):\n",
    "    transformer = 'transformer'\n",
    "    bow = 'bow'\n",
    "    w2v = 'w2v'\n"
   ],
   "metadata": {
    "id": "R8BTRDTOzsxj"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Especificações\n",
    "\n",
    "## Bag of Words (BOW)\n",
    "\n",
    "Inserimos uma sequência de camadas totalmente conectadas (uma DNN padrão) após a vetorização dos dados como modelo de\n",
    "classificação para a label de sentimento.\n",
    "\n",
    "### Hiperparâmetros:\n",
    "\n",
    "- input_dim=2000\n",
    "- hidden_dim=6\n",
    "\n",
    "### Argumentos de treino:\n",
    "\n",
    "- max_epochs=50\n",
    "- batch_size=64\n",
    "- num_workers=8\n",
    "- lr=1e-6\n",
    "\n",
    "## Word To Vector (W2V)\n",
    "\n",
    "Para a implementação do W2V, utilizamos a biblioteca Gensim, que tem uma implementação pronta para a criação de\n",
    "embeddings.\n",
    "\n",
    "Na nossa abordagem utilizamos uma camada LSTM (Long- Short-Term Memory), ou seja, montamos uma rede recorrente como\n",
    "parte do modelo de classificação, além das camadas lineares que tem como entrada a saída do CBOW.\n",
    "\n",
    "### Hiperparâmetros:\n",
    "\n",
    "- embedding_dim=512\n",
    "- hidden_dim=256\n",
    "- num_layers=8\n",
    "\n",
    "### Argumetnos de treino:\n",
    "\n",
    "- max_epochs=20\n",
    "- batch_size=64\n",
    "- num_workers=8\n",
    "- lr=1e-5\n",
    "\n",
    "## Transformer (Transfer Learning baseado em pesos do Bert-Small)\n",
    "\n",
    "O BERT (Bidirectional Encoder Representations from Transformers) é um modelo de Transformer desenvolvido pelo Google,\n",
    "mas escolhemos uma versão com menos pesos, mas a mesma arquitetura. Ele é treinado para entender o contexto de uma\n",
    "palavra observando tanto o que vem antes quanto o que vem depois dela (bidirecional).\n",
    "\n",
    "Isso permite uma interpretação\n",
    "artifical de contexto, o que torna BERT extremamente eficaz para tarefas de NLP e realizar Transfer Learning em cima.\n",
    "Congelamos os pesos do modelo bert-small e inserimos camadas adicionais que foram treinadas em cima dos nossos dados.\n",
    "\n",
    "### Hiperparâmetros:\n",
    "\n",
    "- hidden_layers=3 (apenas as camadas não congeladas)\n",
    "\n",
    "## Argumentos de treino:\n",
    "\n",
    "- max-epochs=10 (o modelo demora bastante para ser treinado)\n",
    "- batch_size=64\n",
    "- num_workers=8\n",
    "- lr=1e-4"
   ],
   "metadata": {
    "id": "M1lX_nVV-inp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AVISO: Todos os modelos foram treinados em máquinas nossas, e demoram bastante para serem treinados na GPU do colab!!!"
   ],
   "metadata": {
    "id": "LMg2whlRDq6-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train('transformer', max_epochs=10, batch_size=64, num_workers=8, lr=1e-4)"
   ],
   "metadata": {
    "id": "XHNIfC0i0Zgy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train('w2v', max_epochs=20, batch_size=64, num_workers=8, lr=1e-5)"
   ],
   "metadata": {
    "id": "LeysTDkyDyyl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train('bow', max_epochs=50, batch_size=64, num_workers=8, lr=1e-6)"
   ],
   "metadata": {
    "id": "Nd4Ic9ZCD41D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resultados e Conclusão\n",
    "\n",
    "Salvamos os pesos, assim como os resultados de teste no nosso GitHub, pois o Google Colab não mantém em memória os arquivos.\n",
    "\n",
    "Acesse aqui: https://github.com/rzimmerdev/sentiment-analysis\n",
    "[ *Última edição 16/06/2024* ]"
   ],
   "metadata": {
    "id": "6iXwFCmbLkVg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, model_dir=None):\n",
    "    dataset = FinancialPhraseDataset()\n",
    "    test_loader = dataset.get_data_loaders(train=False, batch_size=64, num_workers=8, train_size=1)\n",
    "    model_path = f\"{model_dir}/{model}\"\n",
    "\n",
    "    if model == 'bow':\n",
    "        lit_model = LitBowClassifier(2000, 6)\n",
    "        weights = model_path + '.pth'\n",
    "        vectorizer = model_path + '.pkl'\n",
    "        lit_model.load(weights, vectorizer)\n",
    "    elif model == 'w2v':\n",
    "        lit_model = LitWord2VecClassifier(embedding_dim=512, hidden_dim=256, num_layers=8)\n",
    "        weights = model_path + '.pth'\n",
    "        vectorizer = model_path + '.model'\n",
    "        lit_model.load(weights, vectorizer)\n",
    "    elif model == 'transformer':\n",
    "        weights = model_path + '.ckpt'\n",
    "        lit_model = LitTransformerClassifier.load_from_checkpoint(weights, hidden_layers=3)\n",
    "    else:\n",
    "        raise ValueError('Unknown model')\n",
    "\n",
    "    lit_model.eval()\n",
    "\n",
    "    # Calculate metrics\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        output, target, _, _ = lit_model.test_step(batch, idx)\n",
    "\n",
    "        all_outputs.extend(output.tolist())\n",
    "        all_labels.extend(target.tolist())\n",
    "\n",
    "    outputs = np.array(all_outputs)\n",
    "    labels = np.array(all_labels)\n",
    "\n",
    "    pred = np.argmax(outputs, axis=1)\n",
    "    target = np.argmax(labels, axis=1)\n",
    "\n",
    "    # Accuracy (argmax, argmax\n",
    "    test_accuracy = accuracy_score(target, pred)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(target, pred)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(target, pred, average='weighted')\n",
    "\n",
    "    # Log loss\n",
    "    test_loss = log_loss(all_labels, all_outputs)\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    for i in range(3):\n",
    "        fpr, tpr, _ = roc_curve(labels[:, i], outputs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC Curve\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(f'{model}_class_{i}_roc.png')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap='viridis')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(f'{model}_confusion_matrix.png')\n",
    "\n",
    "    # AIC Score (assuming a binomial model, and AIC = 2k - 2ln(L))\n",
    "    # k = num of model params\n",
    "    k = sum(p.numel() for p in lit_model.parameters())\n",
    "    L = -test_loss * len(all_labels)  # log-likelihood\n",
    "    AIC = 2 * k - 2 * L\n",
    "\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'AIC: {AIC}')"
   ],
   "metadata": {
    "id": "NEuppC9-4_0e"
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "Comparando os modelos.\n",
    "\n",
    "Para comparar os modelos, precisamos de uma função que carregue os pesos de um modelo treinado e calcule as métricas de avaliação.\n",
    "As métricas utilizadas são:\n",
    "\n",
    "- Acurácia: a porcentagem de previsões corretas.\n",
    "- Matriz de confusão: uma tabela que mostra o número de previsões corretas e incorretas.\n",
    "- F1 Score: a média ponderada da precisão e recall.\n",
    "- Log Loss: a função de perda logarítmica.\n",
    "- AUC: a área sob a curva ROC (Receiver Operating Characteristic).\n",
    "- AIC: o critério de informação de Akaike.\n",
    "\n",
    "Conclusão:\n",
    "Comparando os três modelos - o BOW como baseline, o Word2Vec e o Transformer - podemos ver que o Transformer obteve a melhor acurácia e F1 Score.\n",
    "O Word2Vec obteve a pior acurácia e F1 Score, mas ainda assim é um modelo razoável.\n",
    "O BOW obteve resultados intermediários, mas é o mais simples dos três modelos.\n",
    "\n",
    "\n",
    "### BOW:\n",
    "\n",
    "Test Accuracy: 0.6876288659793814\n",
    "Test Loss: 3.4750539769234146\n",
    "F1 Score: 0.6822113699098331\n",
    "AIC: 44034747.60471523\n",
    "\n",
    "\n",
    "### W2V:\n",
    "\n",
    "\n",
    "\n",
    "### Transformer:\n",
    "\n",
    "Test Accuracy: 0.8030927835051547\n",
    "Test Loss: 1.7498943486647303\n",
    "F1 Score: 0.7983950697516664\n",
    "AIC: 58845512.79503641\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "evaluate('transformer', 'checkpoints')"
   ],
   "metadata": {
    "id": "bi0PxuNkO6Rn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "evaluate('w2v', 'checkpoints')"
   ],
   "metadata": {
    "id": "11L-cKTnPBJV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "evaluate('bow', 'checkpoints')"
   ],
   "metadata": {
    "id": "dk4iOaxpPBnL"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
